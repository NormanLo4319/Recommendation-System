{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34183494",
   "metadata": {},
   "source": [
    "## Recommendation System\n",
    "\n",
    "Recommendation system is one of the key field for applying machine learning algorithm in most of the technological industries, Netflix (movies, shows, etc), Amazon (books, appliances, etc), Facebook (video, groups, etc), Google (route, recipe, website, etc), and many more. Understanding how to setup a recommendation system is just a starting point getting into the field of data science.  Learning the application of the the tools to solve a recommendation problem will provide you the advantage in the field.\n",
    "\n",
    "Introduction to Recommender System by Andrew Ng:\n",
    "https://www.youtube.com/watch?v=giIXNoiqO_U\n",
    "\n",
    "### Types of Recommendation System: Content-Based vs. Collaborative-Filtering\n",
    "\n",
    "A recommendation system usually has an objective to predict the rating or other ranking or rating metrics for the current users, so the highest predicted rating or ranking content can be recommended to user.\n",
    "\n",
    "**Typical Data Set:** Rating Book from 0 - 5 stars with missing data \"?\"\n",
    "\n",
    "| Book | User_1 | User_2 | User_3 | User_4 | Feature_1 (Sci-Fi) | Feature_2 (Fiction) |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| Book_1 | 5 | 4 | 1 | ? | 0.8 | 0.1 |\n",
    "| Book_2 | 5 | ? | 0 | 0 | 0.9 | 0.05 |\n",
    "| Book_3 | ? | 2 | 4 | 5 | 0.4 | 0.6 |\n",
    "| Book_4 | 0 | 0 | ? | 5 | 0.2 | 0.8 |\n",
    "| Book_5 | 1 | ? | 5 | 4 | 0.13 | 0.75 |\n",
    "\n",
    "The table represents the rating vectors from users and the feature vectors for each book.\n",
    "\n",
    "Note: Features of the content are not always available or well-defined, so this is just one example that the feature of the content is available or defined.\n",
    "\n",
    "#### Content-Based Recommendation\n",
    "\n",
    "**Content-Based** recommendation system utilizes the content features which users react positively to for predicting the behavior of a user. During the recommendation process, the **similarity metric** are calculated from the item's feature vectors and the user's preferred feature vectors from their previous record. Then, the top few are recommended by the system. Content-based filtering does not require other users' data during recommendations to one user.\n",
    "\n",
    "**Pros:**\n",
    "- No data is needed from other users\n",
    "- Individualize prediction based on user profile and can predict unique taste\n",
    "- Well explained recommendation by user profile\n",
    "\n",
    "**Cons:**\n",
    "- Hard to find all appropriate features\n",
    "- Overspecialization\n",
    "- Cold-Start problem, building user profile for first time user\n",
    "\n",
    "Content-Based Recommendations by Andrew Ng:\n",
    "https://www.youtube.com/watch?v=9siFuMMHNIA\n",
    "\n",
    "Let's investigate what to recommend to Bob if we are provided with a feature matrix for some TV shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb297b01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:37:27.075028Z",
     "start_time": "2021-09-17T17:37:25.925189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d225e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:05:47.683075Z",
     "start_time": "2021-09-17T17:05:47.674183Z"
    }
   },
   "outputs": [],
   "source": [
    "tv_shows = np.array([[0, 1, 1, 0, 1, 1, 1],\n",
    "                   [0, 0, 0, 1, 1, 1, 0],\n",
    "                   [1, 1, 1, 0, 0, 1, 1],\n",
    "                   [0, 1, 1, 1, 0, 0, 1]])\n",
    "\n",
    "tv_shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417557a9",
   "metadata": {},
   "source": [
    "Bob likes the TV Show represented by Row \\#1. Which show (row) should we recommend to Bob?\n",
    "\n",
    "**Cosine Similarity**:\n",
    "\n",
    "One natural way of measuring the similarity between two vectors is by the **cosine of the angle between them**. Two points near one another in feature space will correspond to vectors that nearly overlap, i.e. vectors that describe a small angle $\\theta$. And as $\\theta$ decreases, $\\cos(\\theta)$ *increases*. So we'll be looking for large values of the cosine (which ranges between -1 and 1). We can also think of the cosine between two vectors as the *projection of one vector onto the other*:\n",
    "\n",
    "![image.png](https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png)\n",
    "\n",
    "We can use this metric easily if we treat our rows (the items we're comparing for similarity) as vectors: We can calculate the cosine of the angle $\\theta$ between two vectors $\\vec{a}$ and $\\vec{b}$ as follows: \n",
    "\n",
    "$$\\cos(\\theta) = \\frac{\\vec{a}\\cdot\\vec{b}}{|\\vec{a}||\\vec{b}|}$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\vec{a} = \\begin{bmatrix} 3 & 4 \\end{bmatrix}$\n",
    "\n",
    "$\\vec{b} = \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}$\n",
    "\n",
    "$\\vec{a}\\cdot\\vec{b} = 3 \\cdot 4 + 4 \\cdot 3 = 24$\n",
    "\n",
    "$|\\vec{a}| = \\sqrt{3^2 + 4^2} = 5$\n",
    "\n",
    "$|\\vec{b}| = \\sqrt{4^2 + 3^2} = 5$\n",
    "\n",
    "$COS(\\theta) = \\frac{\\vec{a}\\cdot\\vec{b}}{|\\vec{a}||\\vec{b}|} = \\frac{24}{5 \\cdot 5} = 0.96$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ece19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:12:42.151730Z",
     "start_time": "2021-09-17T17:12:42.146543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the denominator and numerators of the Cosine Similarity\n",
    "numerators = np.array([tv_shows[0].dot(tv_show) for tv_show in tv_shows[1:]])\n",
    "denominators = np.array([np.sqrt(sum(tv_shows[0]**2)) *\\\n",
    "                         np.sqrt(sum(tv_show**2)) for tv_show in tv_shows[1:]])\n",
    "\n",
    "# Calculate the Cosine Similarity for each item to item 1\n",
    "similarity = numerators / denominators\n",
    "\n",
    "# Results\n",
    "print(f'Cosine Similarity between Item 1 and 2: {round(similarity[0], 3)}')\n",
    "print(f'Cosine Similarity between Item 1 and 3: {round(similarity[1], 3)}')\n",
    "print(f'Cosine Similarity between Item 1 and 4: {round(similarity[2], 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c473d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:37:29.664957Z",
     "start_time": "2021-09-17T17:37:29.660707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using SKLearm consine_similarity()\n",
    "cosine_similarity(tv_shows, tv_shows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e814d9d",
   "metadata": {},
   "source": [
    "Based on this result, since the cosine similarity to Item 1 is highest for Item 3, we would recommend Item 3 to Bob."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c582e848",
   "metadata": {},
   "source": [
    "#### Collaborative-Filtering Recommendation\n",
    "\n",
    "**Collaborative-Filtering** system does not need the features of the items to be given. Every user and item is described by a feature vector or embedding. It consider other users' reactions while recommending a particular user. It notes which item a particular user likes and also the items that the users with behavior or likings similar to him/her likes, to recommend product to that user.\n",
    "\n",
    "#### Types of Collaborative Recommendation System:\n",
    "\n",
    "##### I. Memory-Based Collaborative Filtering:\n",
    "\n",
    "Done mainly remembering the user-item interaction matrix, and how a user reacts to it, i.e, the rating that a user gives to an item. There is no dimensionality reduction or model fitting as such. Mainly two sections:\n",
    "\n",
    "**Item-Based Filtering**:\n",
    "Recommend item X to a user based on the similarity of other items Y and Z that you liked. \"Because you liked Y and Z, you may also like X.\"\n",
    "\n",
    "**User-Based Filtering**:\n",
    "Recommend item X to a user based on the similarity of characteristics of other users who also like item X. \"The users who like products similar to you also liked those products.\"\n",
    "\n",
    "##### II. Model-Based Collaborative Filtering:\n",
    "\n",
    "From the matrix, we try to learn how a specific user or an item behaves. We compress the large interaction matrix using dimensional Reduction or using clustering algorithms. In this type, We fit machine learning models and try to predict how many ratings will a user give a product. There are several methods:\n",
    "\n",
    "**Clustering Algorithms**: Normally use simple clustering Algorithms like K-Nearest Neighbours to find the K closest neighbors or embeddings given a user or an item embedding based on the similarity metrics used.\n",
    "\n",
    "**Matrix Factorization Based Algorithms**: Like any big number can be factorized into smaller numbers, the user-item interaction table or matrix can also be factorized into two smaller matrices, and these two matrices can also be used to generate back the interaction matrix.\n",
    "\n",
    "**Deep Learning Methods**: Like convolutional network model.\n",
    "\n",
    "Collaborative-Filtering Recommendation by Andrew Ng:\n",
    "https://www.youtube.com/watch?v=9AP-DgFBNP4\n",
    "\n",
    "Let suppose there are three users who like different items,\n",
    "\n",
    "| Items | User_1 | User_2| User_3 |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| Item_1 | 5 | 0 | 0 |\n",
    "| Item_2 | 0 | 5 | 0 |\n",
    "| Item_3 | 0 | 0 | 5 |\n",
    "\n",
    "**User-Based Filtering Example**:\n",
    "\n",
    "What should we recommend to a new user (Bob) based on the similarity of the characteristics between all users. Or, to which is Bob most similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26425e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:30:56.454754Z",
     "start_time": "2021-09-17T17:30:56.451859Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 Users Characteristic Vectors\n",
    "users = np.array([[5, 4, 3, 4, 5],\n",
    "                  [3, 1, 1, 2, 5],\n",
    "                  [4, 2, 3, 1, 4]])\n",
    "\n",
    "# Bob's Characteristic Vector\n",
    "Bob = np.array([5, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c6c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:31:43.071648Z",
     "start_time": "2021-09-17T17:31:43.067311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bob's Characteristic vector\n",
    "Bob_mag = 5\n",
    "\n",
    "# Calculate the numerator and demoninators of cosine similarity\n",
    "numerators = np.array([Bob.dot(user) for user in users])\n",
    "denominators = np.array([Bob_mag * np.sqrt(sum(user**2))\\\n",
    "                         for user in users])\n",
    "\n",
    "# Calculate the cosine similarity\n",
    "similarity = numerators / denominators\n",
    "\n",
    "# Results\n",
    "print(f'Cosine Similarity between Bob and User_1: {round(similarity[0], 3)}')\n",
    "print(f'Cosine Similarity between Bob and User_2: {round(similarity[1], 3)}')\n",
    "print(f'Cosine Similarity between Bob and User_3: {round(similarity[2], 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e503f7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:40:51.157837Z",
     "start_time": "2021-09-17T17:40:51.153209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using SKLearn cosine_similarity()\n",
    "all_users = np.vstack([users, Bob])\n",
    "cosine_similarity(all_users, all_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517c590",
   "metadata": {},
   "source": [
    "Based on this result, we recommend item 3 to Bob because User 3 has the highest cosine similarity score with Bob."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be3482",
   "metadata": {},
   "source": [
    "### Matrix Factorization & Latent Features\n",
    "\n",
    "Suppose we start with a matrix $R$ of users and products, where each cell records the ranking the relevant user gave to the relevant product. Very often we'll be able to record this data as a sparse matrix, because many users will not have ranked many items.\n",
    "\n",
    "**User-Item Rating Matrix**: Matrix R\n",
    "\n",
    "| Users | Item_1 | Item_2 | Item_3 | Item_4 | ... | Item_m |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| User_1 | 5 | ? | 4 | ? | ... | ? |\n",
    "| User_2 | ? | 1 | ? | 5 | ... | ? |\n",
    "| User_3 | 1 | ? | ? | 4 | ... | 4 |\n",
    "| User_4 | ? | ? | 1 | 2 | ... | ? |\n",
    "| User_5 | ? | ? | 5 | ? | ... | 1 |\n",
    "| User_6 | 3 | 5 | ? | ? | ... | ? |\n",
    "| User_7 | 2 | ? | ? | 1 | ... | ? |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| User_n | ? | 4 | 3 | 3 | ... | 5 |\n",
    "\n",
    "Note: User-Item Rating Matrix is usually a very **sparse** matrix, which means that many cells in this matrix are empty. In real-world, a single user does not give ratings to even 1% of the total items. Therefore, around 99% of the cells of this matrix are empty. \n",
    "\n",
    "Imagine factoring this matrix into a user matrix $P$ and an item matrix $Q^T$: $R = PQ^T$. What would the shapes of $P$ and $Q^T$ be? Clearly $P$ must have as many rows as $R$, which is just the number of users who have given ratings. Similarly, $Q^T$ must have as many columns as $R$, which is just the number of items that have received ratings. We also know that the number of columns of $P$ must match the number of rows of $Q^T$ for the factorization to be possible, but this number could really be anything. In practice this will be a small number, and for reasons that will emerge shortly let's refer to these dimensions as **latent features** of the items in $R$. If $p$ is a row of $P$, i.e. a user-vector, and $q$ is a column of $Q^T$, i.e. an item-vector, then $p$ will record the user's particular weights or *preferences* with respect to the latent features, while $q$ will record how the item ranks with respect to those same latent features. This in turn means that we could predict a user's ranking of a particular item simply by calculating the dot-product of $p$ and $q$!\n",
    "\n",
    "**User Matrix**: Matrix P\n",
    "\n",
    "| Users | age | gender | ... | Feature_k |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| User_1 | 25 | 0 | ... | 0.3 |\n",
    "| User_2 | 33 | 1 | ... | 0.55 |\n",
    "| User_3 | 35 | 1 | ... | 0.35 |\n",
    "| User_4 | 28 | 0 | ... | 0.44 |\n",
    "| User_5 | 46 | 0 | ... | 0.65 |\n",
    "| User_6 | 23 | 1 | ... | 0.23 |\n",
    "| User_7 | 41 | 0 | ... | 0.32 |\n",
    "| ... | ... | ... | ... | ... |\n",
    "| User_n | 37 | 1 | ... | 0.68 |\n",
    "\n",
    "**Item Matrix**: Matrix Q\n",
    "\n",
    "| Users | Action | Over 60 minutes | Foreign Language | ... | Feature_k |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| Item_1 | 0 | 1 | 2 | ... | 0 |\n",
    "| Item_2 | 1 | 0 | 4 | ... | 1 |\n",
    "| Item_3 | 1 | 1 | 1 | ... | 1 |\n",
    "| Item_4 | 0 | 1 | 1 | ... | 0 |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "| Item_m | 0 | 0 | 5 | ... | 0 |\n",
    "\n",
    "Note: The columns in the user matrix ($P$) and rows in the transposed item matrix ($Q^T$)are called \"**Latent Factors**\" and are an indication of hidden characteristics about the users or the items. The number of latent factors affects the recommendations in a manner where the greater the number of factors, the more personalized the recommendations become. But too many factors can lead to overfitting in the model.\n",
    "\n",
    "If we could effect such a factorization, $R = PQ^T$, then we could calculate *all* predictions, i.e. fill in the gaps in $R$, by solving for $P$ and $Q$. \n",
    "\n",
    "![matrix factorization](img/matrix_factorization.png)\n",
    "\n",
    "\n",
    "One of the most popular algorithm solving such problem is called \"**Alternating Least-Squares**\" (**ALS**).\n",
    "\n",
    "\n",
    "### Alternating Least Square\n",
    "\n",
    "ALS recommendation systems are often implemented in Spark architectures because of the appropriateness for distributed computing. ALS systems often involve very large datasets (consider how much data the recommendation engine for NETFLIX must have, for example!), and it is often useful to store them as sparse matrices, which Spark's ML library can handle. In fact, Spark's mllib even includes a \"Rating\" datatype! ALS is **collaborative** and **model-based**, and is especially useful for working with *implicit* ratings.\n",
    "\n",
    "We're looking for two matrices (a user matrix and an item matrix) into which we can factor our ratings matrix. We can't of course solve for two matrices at once. But here's what we can do:\n",
    "\n",
    "Make guesses of the values for $P$ and $Q$. Then hold the values of one *constant* so that we can optimize for the values of the other!\n",
    "\n",
    "Basically this converts our problem into a familiar *least-squares* problem. See [this page](https://textbooks.math.gatech.edu/ila/least-squares.html) and [this page](https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/) for more details, but here's the basic idea:\n",
    "\n",
    "#### ALS in `pyspark`\n",
    "\n",
    "We'll talk about Big Data and Spark soon, but I'll just note here that Spark has a recommendation submodule inside its ml (machine learning) module. Source code for `pyspark`'s version [here](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html).\n",
    "\n",
    "#### Non-Negative Matrix Factorization\n",
    "\n",
    "In SKLearn, there is a matrix factorization method called \"Non-Negative Matrix Factorization\". We are using this as an example for making prediction for the user-item rating matrix.\n",
    "\n",
    "SKLearn Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "**Python Code**:\n",
    "\n",
    "``` Python\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create the numpy array to store the rating information\n",
    "R = np.array(R)\n",
    "\n",
    "# Create the NMF object\n",
    "nmf = NMF()\n",
    "\n",
    "# Fit transform the rating in numpy array\n",
    "user = nmf.fit_transform(R)\n",
    "\n",
    "# R\n",
    "item = nmf.components_\n",
    "\n",
    "# Dot product of the user and item matrix\n",
    "pred_rating = np.dot(user,item)\n",
    "\n",
    "# Final predicted rating\n",
    "pred_rating\n",
    "```\n",
    "\n",
    "Here is an example for writing a matrix factorization function, which we are going to compare to the result from NMF model from SKLearn. \n",
    "\n",
    "Source: http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dad414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T05:33:45.018402Z",
     "start_time": "2021-09-19T05:33:45.010958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Matrix Factorization Function\n",
    "import numpy as np\n",
    "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = np.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36543b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T05:33:46.880330Z",
     "start_time": "2021-09-19T05:33:45.714502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the rating matrix\n",
    "R = [\n",
    "     [5,3,0,1],\n",
    "     [4,0,0,1],\n",
    "     [1,1,0,5],\n",
    "     [1,0,0,4],\n",
    "     [0,1,5,4],\n",
    "    ]\n",
    "\n",
    "# Transform the rating matrix into numpy array\n",
    "R = np.array(R)\n",
    "\n",
    "# Setting the parameters\n",
    "N = len(R)\n",
    "M = len(R[0])\n",
    "K = 2\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "\n",
    "# Apply the matrix_factorization() function\n",
    "user, item = matrix_factorization(R, P, Q, K)\n",
    "\n",
    "# Calculate the predict rating by the dot product of the user and item matrix\n",
    "pred_rating = np.dot(user, item.T)\n",
    "print(pred_rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526e29e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T05:37:07.952348Z",
     "start_time": "2021-09-19T05:37:07.943354Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# User-Item Rating Matrix\n",
    "R = [\n",
    "     [5,3,0,1],\n",
    "     [4,0,0,1],\n",
    "     [1,1,0,5],\n",
    "     [1,0,0,4],\n",
    "     [0,1,5,4],\n",
    "    ]\n",
    "\n",
    "# Create the numpy array to store the rating information\n",
    "R = np.array(R)\n",
    "\n",
    "# Create the NMF object\n",
    "nmf = NMF()\n",
    "\n",
    "# Fit transform the rating in numpy array\n",
    "user = nmf.fit_transform(R)\n",
    "\n",
    "# R\n",
    "item = nmf.components_\n",
    "\n",
    "# Dot product of the user and item matrix\n",
    "pred_rating = np.dot(user,item)\n",
    "\n",
    "# Final predicted rating\n",
    "pred_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45bddbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
